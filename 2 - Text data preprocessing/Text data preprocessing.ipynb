{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers. The general purpose of this notebook is to give a practical notion (through this example) of how important data pre-processing can be in a Machine Learning workflow, and generalize it to other situations.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img id=\"fig1\" src=\"https://imgs.xkcd.com/comics/constructive.png\"> \n",
    "\n",
    "A tech company comes to you to create a moderation system for their social network : they want to detect spam comments, and later on also detect offensive contents to remove them automatically.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "To showcase our proposed system, we load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\n",
    "**Questions** :\n",
    "- What simple statistics could you print on the dataset ?\n",
    "- Why is there multiple folders on the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone git@github.com:SupaeroDataScience/machine-learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part7/6-476msg3.txt\n",
      "email is a spam: False\n",
      "Subject: request for discourse list\n",
      "\n",
      "dear linguists i 'd like to know if there are any listservs on discourse anlysis text linguistics and pragmatics . thanks gul durmusoglu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = '../data/lingspam_public/bare/'\n",
    "# train_dir = 'machine-learning/data/ling-spam/train-mails/'\n",
    "\n",
    "email_path = []\n",
    "email_label = []\n",
    "for d in os.listdir(train_dir):\n",
    "    folder = os.path.join(train_dir,d)\n",
    "    email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "    email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00' '000' '0000' '00001' '00003000140' '00003003958' '00007' '0001'\n",
      " '00010' '00014' '0003' '00036' '000bp' '000s' '000yen' '001' '0010'\n",
      " '0010010034' '0011' '00133' '0014' '00170' '0019' '00198' '002' '002656'\n",
      " '0027' '003' '0030' '0031' '00333' '0037' '0039' '003n7' '004' '0041'\n",
      " '0044' '0049' '005' '0057' '006' '0067' '007' '00710' '0073' '0074'\n",
      " '00799' '008' '009' '00919680' '0094' '00a' '00am' '00arrival' '00b'\n",
      " '00coffee' '00congress' '00d' '00dinner' '00f' '00h' '00hfstahlke' '00i'\n",
      " '00j' '00l' '00m' '00p' '00pm' '00r' '00t' '00tea' '00the' '00uzheb' '01'\n",
      " '0100' '01003' '01006' '0104' '0106' '01075' '0108' '011' '0111' '0117'\n",
      " '0118' '01202' '01222' '01223' '01225' '01232' '01235' '01273' '013'\n",
      " '0131' '01334' '0135' '01364' '0139' '013953' '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names_out()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only of nltk is not installed\n",
    "#%pip install nltk\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize      #→ splits text into tokens (words & punctuation).    \n",
    "from nltk.stem import WordNetLemmatizer  #→ reduces words to their base/lemma (e.g., running → run).\n",
    "from nltk.corpus import stopwords        #→ built-in English stop word list.\n",
    "from nltk.corpus import words            #→ dictionary of valid English words from NLTK.\n",
    "from string import punctuation           #→ string of punctuation symbols\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14282\n",
      "Document - words matrix: (2893, 14282)\n",
      "First words: ['aa' 'aal' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abandonment'\n",
      " 'abbas' 'abbreviation' 'abdomen' 'abduction' 'abed' 'aberrant'\n",
      " 'aberration' 'abide' 'abiding' 'abigail' 'ability' 'ablative' 'ablaut'\n",
      " 'able' 'abler' 'aboard' 'abolition' 'abord' 'aboriginal' 'aborigine'\n",
      " 'abound' 'abox' 'abreast' 'abridged' 'abroad' 'abrogate' 'abrook'\n",
      " 'abruptly' 'abscissa' 'absence' 'absent' 'absolute' 'absolutely'\n",
      " 'absoluteness' 'absolutist' 'absolutive' 'absolutization' 'absorbed'\n",
      " 'absorption' 'abstract' 'abstraction' 'abstractly' 'abstractness'\n",
      " 'absurd' 'absurdity' 'abu' 'abundance' 'abundant' 'abuse' 'abusive'\n",
      " 'abyss' 'academe' 'academic' 'academically' 'academician' 'academy'\n",
      " 'accelerate' 'accelerated' 'accelerative' 'accent' 'accentuate'\n",
      " 'accentuation' 'accept' 'acceptability' 'acceptable' 'acceptance'\n",
      " 'acceptation' 'accepted' 'acception' 'access' 'accessibility'\n",
      " 'accessible' 'accessibly' 'accidence' 'accident' 'accidental'\n",
      " 'accidentality' 'accidentally' 'acclaim' 'accommodate' 'accommodation'\n",
      " 'accompany' 'accomplish' 'accomplished' 'accomplishment' 'accord'\n",
      " 'accordance' 'according' 'accordingly' 'account' 'accountability'\n",
      " 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names_out()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: ials ( 6th lang teacher ed )\n",
      "\n",
      "the university of edinburgh institute for applied language studies ( ials ) 6th symposium for language teacher educators evaluation and research in language teacher education - - - - - - - - - - edinburgh - - - - - - - - - - - wednesday 18th november - friday 20th november 1998 call for papers * the role of research and evaluation in language teacher education * methods of researching and evaluating language teacher education * the ethics of evaluation and research in language teacher education * evaluating programmes , trainers , and materials in language teacher education * researching the influence of context on the delivery of language teacher education * researching and evaluating methodologies of language teacher education * research as part of the process of training and teacher development * assessing the development of trainee skills * investigating how teachers change * researching supervision and post-lesson feedback * researching the impact of new technologies in language teacher education _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the ials symposia offer a forum for professional exchange among language teacher educators based in the uk and overseas . numbers are limited to 60 . the themes listed above indicate the possible coverage of the papers . papers should not be limited to reports of research but also reflect on the process of research and evaluation and on their role in language teacher education . if you would like to submit a proposal for a paper on any topic related to the theme of the symposium , please write for a proposal form and further information to : suzie huggins 6th ials symposium for language teacher educators institute for applied language studies university of edinburgh 21 hill place edinburgh eh8 9dp scotland , uk . phone 0131 650 6200 fax 0131 667 5927 email : ials . symposium @ ed . ac . uk\n",
      "\n",
      "Bag of words representation (58 words in dict):\n",
      "{'subject': np.int64(1), 'teacher': np.int64(14), 'university': np.int64(2), 'institute': np.int64(2), 'applied': np.int64(2), 'language': np.int64(14), 'symposium': np.int64(5), 'evaluation': np.int64(4), 'research': np.int64(6), 'education': np.int64(9), 'call': np.int64(1), 'role': np.int64(2), 'ethic': np.int64(1), 'influence': np.int64(1), 'context': np.int64(1), 'delivery': np.int64(1), 'part': np.int64(1), 'process': np.int64(2), 'training': np.int64(1), 'development': np.int64(2), 'trainee': np.int64(1), 'investigating': np.int64(1), 'change': np.int64(1), 'supervision': np.int64(1), 'post': np.int64(1), 'lesson': np.int64(1), 'feedback': np.int64(1), 'impact': np.int64(1), 'new': np.int64(1), 'offer': np.int64(1), 'forum': np.int64(1), 'professional': np.int64(1), 'exchange': np.int64(1), 'among': np.int64(1), 'based': np.int64(1), 'overseas': np.int64(1), 'limited': np.int64(2), 'listed': np.int64(1), 'indicate': np.int64(1), 'possible': np.int64(1), 'coverage': np.int64(1), 'also': np.int64(1), 'reflect': np.int64(1), 'would': np.int64(1), 'like': np.int64(1), 'submit': np.int64(1), 'proposal': np.int64(2), 'paper': np.int64(1), 'topic': np.int64(1), 'related': np.int64(1), 'theme': np.int64(1), 'please': np.int64(1), 'write': np.int64(1), 'form': np.int64(1), 'information': np.int64(1), 'hill': np.int64(1), 'place': np.int64(1), 'phone': np.int64(1)}\n",
      "\n",
      "Vector reprensentation (58 non-zero elements):\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 58 stored elements and shape (1, 14282)>\n",
      "  Coords\tValues\n",
      "  (0, 12156)\t1\n",
      "  (0, 12584)\t14\n",
      "  (0, 13430)\t2\n",
      "  (0, 6486)\t2\n",
      "  (0, 658)\t2\n",
      "  (0, 7020)\t14\n",
      "  (0, 12434)\t5\n",
      "  (0, 4346)\t4\n",
      "  (0, 10562)\t6\n",
      "  (0, 3965)\t9\n",
      "  (0, 1733)\t1\n",
      "  (0, 10823)\t2\n",
      "  (0, 4317)\t1\n",
      "  (0, 6384)\t1\n",
      "  (0, 2688)\t1\n",
      "  (0, 3251)\t1\n",
      "  (0, 8913)\t1\n",
      "  (0, 9757)\t2\n",
      "  (0, 12976)\t1\n",
      "  (0, 3403)\t2\n",
      "  (0, 12974)\t1\n",
      "  (0, 6678)\t1\n",
      "  (0, 1980)\t1\n",
      "  (0, 12300)\t1\n",
      "  (0, 9502)\t1\n",
      "  :\t:\n",
      "  (0, 480)\t1\n",
      "  (0, 1126)\t1\n",
      "  (0, 8763)\t1\n",
      "  (0, 7238)\t2\n",
      "  (0, 7285)\t1\n",
      "  (0, 6305)\t1\n",
      "  (0, 9499)\t1\n",
      "  (0, 2890)\t1\n",
      "  (0, 428)\t1\n",
      "  (0, 10339)\t1\n",
      "  (0, 14191)\t1\n",
      "  (0, 7225)\t1\n",
      "  (0, 12174)\t1\n",
      "  (0, 9845)\t2\n",
      "  (0, 8856)\t1\n",
      "  (0, 12896)\t1\n",
      "  (0, 10421)\t1\n",
      "  (0, 12723)\t1\n",
      "  (0, 9346)\t1\n",
      "  (0, 14201)\t1\n",
      "  (0, 4980)\t1\n",
      "  (0, 6391)\t1\n",
      "  (0, 5824)\t1\n",
      "  (0, 9309)\t1\n",
      "  (0, 9195)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** : \n",
    "- What is a bag-of-word representation ?\n",
    "- What kind of feature selection or feature engineering could you derivate from this bag of word representation ?\n",
    "\n",
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect — the phrase in your screenshot refers to **TF–IDF**:\n",
    "\n",
    "---\n",
    "\n",
    "## **TF–IDF = Term Frequency × Inverse Document Frequency**\n",
    "\n",
    "It’s a statistical measure used in **text mining and NLP** to evaluate how important a word is to a document within a collection (corpus).\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Term Frequency (TF)**\n",
    "\n",
    "* How often a word appears in a document.\n",
    "* Formula:\n",
    "  [\n",
    "  TF(t,d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total terms in document d}}\n",
    "  ]\n",
    "\n",
    "👉 Example:\n",
    "In document *d1*:\n",
    "*\"the cat sat on the mat\"*\n",
    "\n",
    "* `cat` appears 1 time, total words = 6\n",
    "* TF(`cat`, d1) = 1/6\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "* Measures how unique or rare a word is across the corpus.\n",
    "* Formula:\n",
    "  [\n",
    "  IDF(t) = \\log \\frac{N}{1 + n_t}\n",
    "  ]\n",
    "  where:\n",
    "\n",
    "  * *N* = total number of documents\n",
    "  * *nₜ* = number of documents containing term *t*\n",
    "\n",
    "👉 Meaning:\n",
    "\n",
    "* If a word appears in **all documents**, IDF ≈ 0 (not useful for distinguishing).\n",
    "* If a word appears in **few documents**, IDF is high (more important).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. TF–IDF Score**\n",
    "\n",
    "[\n",
    "TF\\text{–}IDF(t,d) = TF(t,d) \\times IDF(t)\n",
    "]\n",
    "\n",
    "👉 Interpretation:\n",
    "\n",
    "* High TF–IDF = word is frequent in the document but rare across other documents → very informative.\n",
    "* Low TF–IDF = word is either common everywhere (*the, is, and*) or not frequent → not informative.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Corpus = 2 documents:\n",
    "\n",
    "* d1: \"cat sat on mat\"\n",
    "\n",
    "* d2: \"dog sat on log\"\n",
    "\n",
    "* Word **“sat”** → appears in both documents → IDF low.\n",
    "\n",
    "* Word **“cat”** → only in d1 → IDF high.\n",
    "\n",
    "So:\n",
    "\n",
    "* TF–IDF(\"cat\", d1) > TF–IDF(\"sat\", d1).\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Why use TF–IDF?**\n",
    "\n",
    "* It’s the foundation of **search engines** (ranking documents by relevance).\n",
    "* Helps filter out **common words** (like stopwords) without needing a predefined list.\n",
    "* Used in **text classification, clustering, and keyword extraction**.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **extend your CountVectorizer code** to use `TfidfVectorizer` with your custom tokenizer, so you see the actual TF–IDF scores for words in your dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14282)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 58 stored elements and shape (1, 14282)>\n",
      "  Coords\tValues\n",
      "  (0, 428)\t0.021668194656170932\n",
      "  (0, 480)\t0.03965204853794966\n",
      "  (0, 658)\t0.07930409707589932\n",
      "  (0, 1126)\t0.029642508334120857\n",
      "  (0, 1733)\t0.02491156311317319\n",
      "  (0, 1980)\t0.03686932232020423\n",
      "  (0, 2688)\t0.039045919819268096\n",
      "  (0, 2890)\t0.0584370988942504\n",
      "  (0, 3251)\t0.05255481414582111\n",
      "  (0, 3403)\t0.06891276945435791\n",
      "  (0, 3965)\t0.3709094897191037\n",
      "  (0, 4317)\t0.07106820000914878\n",
      "  (0, 4346)\t0.1750506972872621\n",
      "  (0, 4407)\t0.048629512469554305\n",
      "  (0, 4694)\t0.06178348135429918\n",
      "  (0, 4980)\t0.029559492528951074\n",
      "  (0, 5018)\t0.042615690094226964\n",
      "  (0, 5824)\t0.055212812782293695\n",
      "  (0, 6138)\t0.05683061305812067\n",
      "  (0, 6305)\t0.04530175029244509\n",
      "  (0, 6384)\t0.05014336855822714\n",
      "  (0, 6391)\t0.01970221977995616\n",
      "  (0, 6486)\t0.07045254621446256\n",
      "  (0, 6678)\t0.06393903942186312\n",
      "  (0, 7020)\t0.26167024631502506\n",
      "  :\t:\n",
      "  (0, 8913)\t0.03208251597430934\n",
      "  (0, 9195)\t0.030352501688857378\n",
      "  (0, 9309)\t0.031901814030033504\n",
      "  (0, 9346)\t0.021099582342084266\n",
      "  (0, 9499)\t0.03123146481818496\n",
      "  (0, 9502)\t0.0383325638437251\n",
      "  (0, 9757)\t0.07799378100628432\n",
      "  (0, 9778)\t0.047195767690286015\n",
      "  (0, 9845)\t0.1000178249359564\n",
      "  (0, 10339)\t0.0565843547424653\n",
      "  (0, 10421)\t0.03332125256169084\n",
      "  (0, 10562)\t0.14842253887653006\n",
      "  (0, 10823)\t0.08004350442971138\n",
      "  (0, 12156)\t0.010958033873243352\n",
      "  (0, 12174)\t0.036281093424061064\n",
      "  (0, 12300)\t0.07106820000914878\n",
      "  (0, 12434)\t0.23247507699982753\n",
      "  (0, 12584)\t0.7430165800207059\n",
      "  (0, 12723)\t0.04324603832681238\n",
      "  (0, 12896)\t0.037072631192389925\n",
      "  (0, 12974)\t0.08310683068193532\n",
      "  (0, 12976)\t0.051742736476683246\n",
      "  (0, 13430)\t0.036806285323039215\n",
      "  (0, 14191)\t0.023830317172426497\n",
      "  (0, 14201)\t0.04062915389496815\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part7/6-476msg3.txt\n",
      "email is a spam: False\n",
      "Subject: request for discourse list\n",
      "\n",
      "dear linguists i 'd like to know if there are any listservs on discourse anlysis text linguistics and pragmatics . thanks gul durmusoglu\n",
      "\n",
      "Bag of words representation (12 words in dictionary):\n",
      "{'subject': np.int64(1), 'like': np.int64(1), 'thanks': np.int64(1), 'linguistics': np.int64(1), 'request': np.int64(1), 'know': np.int64(1), 'discourse': np.int64(2), 'pragmatic': np.int64(1), 'list': np.int64(1), 'dear': np.int64(1), 'text': np.int64(1), 'gul': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
