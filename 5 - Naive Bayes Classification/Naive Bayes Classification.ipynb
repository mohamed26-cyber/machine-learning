{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Bayesian models for Machine Learning<br>Naive Bayes Classification</div>\n",
    "\n",
    "One very common application of naive Bayes classifiers is document classification (e-mail spam filtering, sentiment analysis on social networks, technical documentation classification, customer appreciations, etc.). \n",
    "\n",
    "Naive Bayes classifiers for documents estimate the probability of a given document belonging to a certain class Y of documents, based on the document's contents Xi.\n",
    "\n",
    "\n",
    "Suppose we want to predict the probability that sample $x$ has label $y$. This is a probability estimation problem that can be written:\n",
    "$$\\mathbb{P}(Y=y|X=x)$$\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "$$\\mathbb{P}(Y=y|X=x) =\\frac{\\mathbb{P}(X=x|Y=y)\\cdot\\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}$$\n",
    "$$\\textrm{posterior} = \\frac{\\textrm{likelihood}\\cdot\\textrm{prior}}{\\textrm{evidence}}$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Bayesian inference** is the problem of estimating this **posterior distribution**.<br>\n",
    "In plain words, it consists in estimating the probability of label $y$, given an input $x$, using previously seen data to estimate the **likelihood** of an $x$ input associated to label $y$ and the general **prior** probability of observing label $y$.\n",
    "</div>\n",
    "\n",
    "Note that Bayesian inference applies both to classification and regression.\n",
    "\n",
    "The goal of Bayesian inference is to estimate the label distribution for a given $x$ and use them to predict the correct label, so it is a *probabilistic approach to Machine Learning*.\n",
    "\n",
    "The Bayesian predictor (classifier or regressor) returns the label that maximizes the posterior probability distribution.\n",
    "\n",
    "In this (first) notebook on Bayesian modeling in ML, we will explore the method of Naive Bayes Classification.\n",
    "\n",
    "1. [The naive Bayes assumption](#sec1)\n",
    "2. [Naive Bayes classifiers in scikit-learn](#sec2)\n",
    "3. [Examples](#sec3)\n",
    "    1. [The \"spam or ham?\" example](#sec3-1)\n",
    "    2. [The NIST example](#sec3-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a>The naive Bayes assumption\n",
    "\n",
    "Let's start with some illustrative data. We consider an artificial data set of 9 individuals. The first column in our data set is the sex ($S=0$ for male, 1 for female), the second is the height $H$ (in meters), the third is the weight $W$ (in kilos) and the last is the foot size $F$ (in centimeters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.82, 82.  , 30.  ],\n",
       "       [ 0.  ,  1.8 , 86.  , 28.  ],\n",
       "       [ 0.  ,  1.7 , 77.  , 30.  ],\n",
       "       [ 0.  ,  1.8 , 75.  , 25.  ],\n",
       "       [ 1.  ,  1.52, 45.  , 15.  ],\n",
       "       [ 1.  ,  1.65, 68.  , 20.  ],\n",
       "       [ 1.  ,  1.68, 59.  , 18.  ],\n",
       "       [ 1.  ,  1.75, 68.  , 23.  ],\n",
       "       [ 1.  ,  1.58, 49.  , 19.  ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size=(10, 10)\n",
    "\n",
    "data = np.loadtxt(\"sex_classif.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "- Using matplotlib, bokeh, seaborn or plotly, plot one relevant figure on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a0904875-2895-4fe5-b652-1614ba0b738d\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"a0904875-2895-4fe5-b652-1614ba0b738d\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a0904875-2895-4fe5-b652-1614ba0b738d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"c472957e-a03a-45ab-b843-ed488d1034fd\" data-root-id=\"p1148\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"5e4ecb6e-42d7-4305-8260-20fd1d88a22c\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1193\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1194\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1148\",\"attributes\":{\"width\":700,\"height\":500,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1149\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1150\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1158\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1159\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1151\",\"attributes\":{\"text\":\"Scatter plot H vs W selon le sexe\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1188\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1145\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1146\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1147\"},\"data\":{\"type\":\"map\",\"entries\":[[\"H\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/2NgAIKGEAcGMB0KoR2CIfSBIKi4G4RmCITyfdH4Hg4A/p7WF0gAAAA=\"},\"shape\":[9],\"dtype\":\"float64\",\"order\":\"little\"}],[\"W\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/2NgAAE7BzDFYAOlYXxLKK0HpU2gtBGUNofSxg4AYqLB5EgAAAA=\"},\"shape\":[9],\"dtype\":\"float64\",\"order\":\"little\"}],[\"sex\",{\"type\":\"ndarray\",\"array\":[\"Homme\",\"Homme\",\"Homme\",\"Homme\",\"Femme\",\"Femme\",\"Femme\",\"Femme\",\"Femme\"],\"shape\":[9],\"dtype\":\"object\",\"order\":\"little\"}],[\"color\",[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1189\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1190\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1185\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"H\"},\"y\":{\"type\":\"field\",\"field\":\"W\"},\"size\":{\"type\":\"value\",\"value\":8},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.7},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1186\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"H\"},\"y\":{\"type\":\"field\",\"field\":\"W\"},\"size\":{\"type\":\"value\",\"value\":8},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1187\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"H\"},\"y\":{\"type\":\"field\",\"field\":\"W\"},\"size\":{\"type\":\"value\",\"value\":8},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1157\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1170\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1171\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1172\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1173\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1179\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1178\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1180\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1181\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1165\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1166\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1167\"},\"axis_label\":\"W\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1168\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1160\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1161\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1162\"},\"axis_label\":\"H\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1163\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1164\",\"attributes\":{\"axis\":{\"id\":\"p1160\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1169\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1165\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1191\",\"attributes\":{\"location\":\"top_left\",\"title\":\"Sexe\",\"border_line_alpha\":0.3,\"background_fill_alpha\":0.3,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1192\",\"attributes\":{\"label\":{\"type\":\"field\",\"field\":\"sex\"},\"renderers\":[{\"id\":\"p1188\"}]}}]}}]}}]}};\n  const render_items = [{\"docid\":\"5e4ecb6e-42d7-4305-8260-20fd1d88a22c\",\"roots\":{\"p1148\":\"c472957e-a03a-45ab-b843-ed488d1034fd\"},\"root_ids\":[\"p1148\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1148"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "\n",
    "# Exemple : Y = sexe (0=male,1=female)\n",
    "Y = data[:, 0].astype(int)\n",
    "X = data[:, 1:4]\n",
    "\n",
    "# Pour affichage dans Jupyter\n",
    "output_notebook()\n",
    "\n",
    "# Préparer les données dans une ColumnDataSource\n",
    "sex_labels = [\"Homme\", \"Femme\"]\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "sex = np.array([sex_labels[int(i)] for i in Y])\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    H=X[:, 1],\n",
    "    W=X[:, 2],\n",
    "    sex=sex,\n",
    "    color=[colors[i] for i in Y]\n",
    "))\n",
    "\n",
    "# Créer la figure\n",
    "p = figure(\n",
    "    title=\"Scatter plot H vs W selon le sexe\",\n",
    "    x_axis_label=\"H\",\n",
    "    y_axis_label=\"W\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "# Utiliser scatter() au lieu de circle()\n",
    "p.scatter(\n",
    "    x=\"H\",\n",
    "    y=\"W\",\n",
    "    size=8,\n",
    "    color=\"color\",\n",
    "    alpha=0.7,\n",
    "    legend_field=\"sex\",\n",
    "    source=source\n",
    ")\n",
    "\n",
    "# Personnaliser la légende\n",
    "p.legend.title = \"Sexe\"\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.background_fill_alpha = 0.3\n",
    "p.legend.border_line_alpha = 0.3\n",
    "\n",
    "# Afficher le graphique\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to answer the question: is $(H=1.81, W=59, F=21)$ male or female?\n",
    "\n",
    "Let's try to estimate $\\mathbb{P}(S=0|H=1.81, W=59, F=21)$.\n",
    "\n",
    "According to Bayes' theorem, the probability that a person that measures 1.81m, weights 59kgs and has a foot size of 21cm is male, is actually the likelihood of observing a person with such features among males, multiplied by the probability of observing males in the population, divided by the probability of observing an individual with these features.\n",
    "\n",
    "That's a long sentence. Let's write that mathematically:\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81, W=59, F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "Let's make that more readable and more general:\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H,W,F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H,W,F)}$$\n",
    "\n",
    "Interestingly, since our goal is only to compare the probabilities for $S=0$ and $S=1$, the denominator in the last equation won't be relevant. So we are left with two terms to estimate, given the available data:\n",
    "- $\\mathbb{P}(S=0)$: the prior - the probability that any individual is $S=0$, regardless of his/her physical attributes;\n",
    "- $\\mathbb{P}(H=1.81, W=59, F=21 | S=0)$: the likelihood of meeting somebody with the specified features, given that his/her sex is $S=0$.\n",
    "\n",
    "The prior, in this case, is easy to estimate by comparing the frequencies of male and female individuals in the population.\n",
    "\\begin{gather*}\n",
    "\\mathbb{P}(S=0) = \\frac{4}{9}\\\\\n",
    "\\mathbb{P}(S=1) = \\frac{5}{9}\n",
    "\\end{gather*}\n",
    "Technically, the estimate above is obtained by [*maximum likelihood estimation*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "The likelihood, however, is a bit trickier. Can we directly estimate the **joint probability** of the 3 variables $(H,W,F)$?\n",
    "\n",
    "Theoretically, we can. We can assume that among male individuals, $(H,W,F)$ are distributed according to a multivariate Normal distribution, with mean $\\mu=(\\mu_H, \\mu_W, \\mu_F)$ and covariance matrix $\\Sigma$. The trick is then to estimate $\\mu$ and $\\Sigma$.\n",
    "\n",
    "As a matter of fact, estimating $\\mu$ and $\\Sigma$ without further hypothesis would require quite a lot of data, especially because $\\Sigma$ captures the **correlation** between $H$, $W$ and $F$.\n",
    "\n",
    "$\\Sigma$ is a $3\\times 3$ matrix, so it involves 9 parameters to estimate, and we unfortunately only have 9 data points.\n",
    "\n",
    "Let's rephrase this from another perspective. With some basic probabilities, we have:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S, H) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S, H, W)\n",
    "\\end{align*}\n",
    "\n",
    "Those three probabilities are univariate probabilities, much easier to estimate. However, the first one is a function of $S$ only, the second one depends on $S$ and $H$ and the third one depends on $S$, $H$ and $W$. To get an accurate estimate of the third one, we would need samples of the distribution of $F$ in enough points in the space of $(S,H,W)$ to cover it reasonably. This would require a number of data points that is exponential in the number of variables. That's what is called the **curse of dimensionality**, which makes this estimation problem difficult.\n",
    "\n",
    "Let's make this concrete. Suppose we discretize $H$, $W$ and $F$ in 10 bins each and suppose we require 100 samples to get a correct estimate of $\\mathbb{P}(F | S, H, W)$ for any given value of $(F, S, H, W)$. Then we need $100\\cdot 10^3\\cdot 2$ samples to correctly estimate this probability for all possible values of $(F, S, H, W)$. More generally, if we had $n$ continuous features rather than just three, we would require a number of data points that is exponential in $n$.\n",
    "\n",
    "To circumvent this problem, we are going to make a very **naive** assumption (hence the name of the method). We are going to assume that the weight, the height and the foot size are totally independent variables, that is the probability that a person be 1.85m is the same whatever his/her weight and foot size.\n",
    "\n",
    "Obviously, this hypothesis is very strong and clearly does not hold is most real-world cases. But we will assume it nonetheless. In this case, the likelihood estimation becomes:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S)\n",
    "\\end{align*}\n",
    "\n",
    "Each of these probabilities now only depends on the label $S$ and is much easier to estimate from the data. This **conditional independence** assumption is called the **naive Bayes hypothesis**. It allow us to give a (very bad) estimate of $\\mathbb{P}(X | Y)$ and hence of $\\mathbb{P}(Y|X)$.\n",
    "\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H | S)\\cdot \\mathbb{P}(W | S) \\cdot \\mathbb{P}(F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H, W, F)}$$\n",
    "\n",
    "Or, in our case:\n",
    "\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81 | S=0)\\cdot \\mathbb{P}(W=59 | S=0) \\cdot \\mathbb{P}(F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "The **naive Bayes classifier** is then the classifier that estimates all class probabilities and returns the one with maximum probability.\n",
    "\n",
    "$$f(H, W, F) = \\arg\\max_{s} \\mathbb{P}(S=s|H,W,F) = \\arg\\max_{s} \\mathbb{P}(H|S=s)\\cdot \\mathbb{P}(W|S=s) \\cdot \\mathbb{P}(F|S=s)\\cdot \\mathbb{P}(S=s)$$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Let's implement a naive Bayes classifier on the data above, just to practice. We will assume that the $\\mathbb{P}(X | S)$ distributions are Gaussians (for $X = H,W,$ or $F$). Compute the scores and probabilities for each sex, for $(H=1.81, W=59, F=21)$.<br>\n",
    "Hint: use the `np.mean` and `np.std` functions to estimate distribution parameters. Use `scipy.stats.norm.pdf` to compute the Gaussian probability density function in a given input.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score male    : 6.981284895980458e-10\n",
      "score female  : 0.0012161942837264688\n",
      "proba male    : 5.740267802562792e-07\n",
      "proba female  : 0.9999994259732197\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "# Estimate distribution parameters for males\n",
    "dataM = data[data[:,0]==0]\n",
    "mu_HS0 = np.mean(dataM[:,1])\n",
    "std_HS0 = np.std(dataM[:,1])\n",
    "mu_WS0 = np.mean(dataM[:,2])\n",
    "std_WS0 = np.std(dataM[:,2])\n",
    "mu_FS0 = np.mean(dataM[:,3])\n",
    "std_FS0 = np.std(dataM[:,3])\n",
    "pS0 = dataM.shape[0]/data.shape[0]\n",
    "\n",
    "# Estimate distribution parameters for females\n",
    "dataF = data[data[:,0]==1]\n",
    "mu_HS1 = np.mean(dataF[:,1])\n",
    "std_HS1 = np.std(dataF[:,1])\n",
    "mu_WS1 = np.mean(dataF[:,2])\n",
    "std_WS1 = np.std(dataF[:,2])\n",
    "mu_FS1 = np.mean(dataF[:,3])\n",
    "std_FS1 = np.std(dataF[:,3])\n",
    "pS1 = dataF.shape[0]/data.shape[0]\n",
    "\n",
    "# score that (H=1.81,W=59,F=21) is male/female\n",
    "H=1.81\n",
    "W=59\n",
    "F=21\n",
    "from scipy.stats import norm\n",
    "score_M = pS0 * norm.pdf(H,mu_HS0,std_HS0) * norm.pdf(W,mu_WS0,std_WS0) * norm.pdf(F,mu_FS0,std_FS0)\n",
    "score_F = pS1 * norm.pdf(H,mu_HS1,std_HS1) * norm.pdf(W,mu_WS1,std_WS1) * norm.pdf(F,mu_FS1,std_FS1)\n",
    "print(\"score male    :\", score_M)\n",
    "print(\"score female  :\", score_F)\n",
    "print(\"proba male    :\", score_M/(score_M+score_F))\n",
    "print(\"proba female  :\", score_F/(score_M+score_F))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we will always multiply together values that are smaller than one. The result will quickly become very small. It is a good habit to move to log-scale.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**<br>\n",
    "Reuse your code above to compute log scores instead of scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log score male:     -21.08261794758859\n",
      "log score female:   -6.712028735399105\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "log_score_M = np.log(pS0) + norm.logpdf(H,mu_HS0,std_HS0) + norm.logpdf(W,mu_WS0,std_WS0) + norm.logpdf(F,mu_FS0,std_FS0)\n",
    "log_score_F = np.log(pS1) + norm.logpdf(H,mu_HS1,std_HS1) + norm.logpdf(W,mu_WS1,std_WS1) + norm.logpdf(F,mu_FS1,std_FS1)\n",
    "print(\"log score male:    \", log_score_M)\n",
    "print(\"log score female:  \", log_score_F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: $(H=1.81,W=59,F=21)$ is most probably female.\n",
    "\n",
    "Let's generalize.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "    \n",
    "Given $n$ features $X_i$ and classes $Y$, **naive Bayes classifiers** estimate (from data) the distributions $\\mathbb{P}(Y)$ and $\\mathbb{P}(X_i|Y)$. Then, using Bayes rule and the naive Bayes assumption, they predict the most probable estimated class:\n",
    "\\begin{align*}\n",
    "\\arg\\max_{y} \\mathbb{P}(Y=y|X=x) & = \\arg\\max_{y} \\frac{\\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}\\\\\n",
    "& = \\arg\\max_{y} \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)\\\\\n",
    "& = \\arg\\max_{y} \\sum\\limits_{i=1}^n \\log\\left(\\mathbb{P}(X_i=x_i|Y=y)\\right) + \\log\\left(\\mathbb{P}(Y=y)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that although it is not compulsory to compute the denominator, it is quite straightforward since:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(X=x) &= \\sum\\limits_y \\mathbb{P}(X=x|Y=y)\\mathbb{P}(Y=y)\\\\\n",
    "&= \\sum\\limits_y \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y) \n",
    "\\end{align*}\n",
    "So it's the sum of the numerator's values for all $y$, so it's just a matter of normalizing the scores obtained.\n",
    "\n",
    "A really nice thing about naive Bayes classifiers is that it is an **online method**, since most univariate probability distributions can be updated incrementally, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. Naive Bayes classifiers in scikit-learn\n",
    "\n",
    "Once again, scikit-learn has a [naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) implementation. It allows three kind of distributions for the $X_i|Y$ variables: Normal (continuous), Bernouilli or Multinomial (discrete).\n",
    "Let's directly use it on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [1.]\n",
      "Probas:      [[5.73982396e-07 9.99999426e-01]]\n",
      "Log probas:  [[-1.43706671e+01 -5.73982561e-07]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "gnb.fit(X,y)\n",
    "xtest = np.array([[1.81,59,21]])\n",
    "print(\"Prediction: \", gnb.predict(xtest))\n",
    "print(\"Probas:     \", gnb.predict_proba(xtest))\n",
    "print(\"Log probas: \",gnb.predict_log_proba(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. Examples\n",
    "\n",
    "## <a id=\"sec3-1\"></a> 3.1 The \"spam or ham?\" example\n",
    "\n",
    "Let's scale up and apply naive Bayes classification on the ling-spam data. We will assume a multinomial distribution of word $i$ appearing in and email of class $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "print(\"data loaded\")\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**\n",
    "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the data above. Estimate its generalization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9473684210526315\n",
      "******************** done!\n",
      "Average generalization score: 0.9232362821948487\n",
      "Standard deviation: 0.008626800516698833\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "print(\"Score:\", spam_nbc.score(Xtest,ytest))\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our model in the Tf-Idf data. Let's see how the model behaves on raw word counts.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**\n",
    "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the raw word counts data below. Estimate its generalization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9339305711086227\n",
      "******************** done!\n",
      "Average generalization score: 0.9882418812989922\n",
      "Standard deviation: 0.0030972713742305146\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "print(\"Score:\", spam_nbc.score(Xtest,ytest))\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000,feat='wordcount')\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify which are the misclassified emails (and find the confusion matrix by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified messages indices: [8, 100, 294, 383, 384, 450, 566, 598, 611, 628, 691, 725, 736, 737, 784, 807, 828]\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_nbc.predict(Xtest)\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[733  13]\n",
      " [  4 143]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [False]\n",
      "email file: ../data/lingspam_public/bare/part5/spmsgb164.txt\n",
      "email is a spam: True\n",
      "Subject: university degree programs\n",
      "\n",
      "university degree programs increase your personal prestige and money earning power through an advanced university degree . eminent , non-accredited universities will award you a degree for only $ 200 . degree granted based on your present knowledge and experience . no further effort necessary on your part . just a short phone call is all that is required for a ba , ma , mba , or phd diploma in the field of your choice . for details , call 770-492 - 2925\n",
      "\n",
      "Bag of words representation (28 words in dictionary):\n",
      "{'subject': np.int64(1), 'university': np.int64(3), 'call': np.int64(2), 'part': np.int64(1), 'based': np.int64(1), 'phone': np.int64(1), 'money': np.int64(1), 'non': np.int64(1), 'power': np.int64(1), 'field': np.int64(1), 'experience': np.int64(1), 'necessary': np.int64(1), 'knowledge': np.int64(1), 'present': np.int64(1), 'short': np.int64(1), 'personal': np.int64(1), 'choice': np.int64(1), 'increase': np.int64(1), 'effort': np.int64(1), 'advanced': np.int64(1), 'degree': np.int64(5), 'earning': np.int64(1), 'award': np.int64(1), 'diploma': np.int64(1), 'ba': np.int64(1), 'prestige': np.int64(1), 'accredited': np.int64(1), 'eminent': np.int64(1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.39310455, 0.60689545]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[1]+2000\n",
    "print(\"Prediction:\", spam_nbc.predict(spam_data.word_count[index,:]))\n",
    "spam_data.print_email(index)\n",
    "spam_nbc.predict_proba(spam_data.tfidf[index,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "- What are the next steps to improve the results ? To create the moderation system ?\n",
    "- What questions should you ask to the tech company, before working on their data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-2\"></a> 3.2 The NIST example\n",
    "\n",
    "We will assume Gaussian distributions for the NIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 8, 8)\n",
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "#Xtrain = X[:1000,:]\n",
    "#ytrain = y[:1000]\n",
    "#Xtest = X[1000:,:]\n",
    "#ytest = y[1000:]\n",
    "\n",
    "#print(digits.DESCR)\n",
    "\n",
    "#plt.gray();\n",
    "#plt.matshow(digits.images[0]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[15]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[42]);\n",
    "#plt.show();\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64)\n",
      "(1000,)\n",
      "Generalization error: 0.18695106649937265\n",
      "Generalization score: 0.8130489335006273\n",
      "Confusion matrix:\n",
      "[[83  0  0  0  0  1  0  0  0  0]\n",
      " [ 0 75  1  0  0  0  0  1  2  4]\n",
      " [ 0 10 57  0  1  0  1  0 14  0]\n",
      " [ 0  1  1 65  0  5  0  3  8  3]\n",
      " [ 1  3  0  0 55  0  3 18  0  0]\n",
      " [ 0  1  0  1  0 76  1  2  2  0]\n",
      " [ 0  2  0  0  0  0 67  0  0  0]\n",
      " [ 0  0  1  0  3  2  0 75  0  0]\n",
      " [ 0 17  1  1  0  2  0  6 46  0]\n",
      " [ 0  4  0  3  1  5  0  6  7 49]]\n"
     ]
    }
   ],
   "source": [
    "Xtrain, ytrain, Xtest, ytest  = shuffle_and_split(X,y,1000)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_nbc = GaussianNB()\n",
    "digits_nbc.fit(Xtrain,ytrain)\n",
    "prediction = digits_nbc.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_nbc.score(Xtest,ytest))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les colonnes montrent les vrais observations\n",
    "### Les lignes montrent les observations prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** done!\n",
      "Average generalization score: 0.8337515683814303\n",
      "Standard deviation: 0.02045785098717688\n"
     ]
    }
   ],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = shuffle_and_split(X,y,1000)\n",
    "    digits_nbc = GaussianNB()\n",
    "    digits_nbc.fit(Xtrain,ytrain)\n",
    "    score += [digits_nbc.score(Xtest,ytest)]\n",
    "    print('*',end='')\n",
    "print(\" done!\")\n",
    "    \n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers reach their limits on data with high correlations between features (like images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
