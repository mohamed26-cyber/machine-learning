{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>\n",
    "\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">XGBoost<br>Introduction to XGBoost</div>\n",
    "\n",
    "This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour :\n",
    "* In the **first notebook**, you learned the **basic of XGBoost**, how to apply it on a dataset and tune it to obtain the best performances.\n",
    "* In the **second notebook**, we focused on **ensemble methods**.\n",
    "* Finally in the **last notebook** you will see how the choice of a method (such as XGBoost) is a key element of a tradeoff between **Bias and Variance**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by a few reminder on the Bias/Variance tradeoff :\n",
    "\n",
    "\n",
    "**Bias** is the mean error between our prediction and the correct value. A high bias means that our model makes large mistakes, even on the training dataset.\n",
    "\n",
    "**Variance** is the sensitivity of the prediction to small changes in the dataset. A high variance means that the model performs well on the training dataset, but poorly on the test dataset.\n",
    "\n",
    "Let's summarize that with a figure :\n",
    "\n",
    "<img src=\"img/1 xwtSpR_zg7j7zusa4IDHNQ.png\">\n",
    "\n",
    "In this figure, the center of the target is what we try to predict - as if we were trying to send an arrow in the bullseye zone.\n",
    "A model with a high bias cannot reach the center - the center of gravity of all its arrows is on the outer part of the target.\n",
    "A model with a low bias is globally centered on the target.\n",
    "\n",
    "A model with a high variance is not consistant - its arrows are largely spread.\n",
    "A model with low variance is very consistant - even when its arrows are not in the right place, they are in a limited zone.\n",
    "\n",
    "In a machine learning algorithm, it is a *tradeoff* between Bias and Variance : we want to minimize both (low bias, low variance), but usually when one decreases the other increases.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control overfitting in Gradient Boosted Trees, we can use 3 main leverages : tree structure, shrinkage and randomization.\n",
    "\n",
    "We already saw that the tree parameters, especially those related to tree depth such as max_depth and gamma, is a way to control the complexity of the model, and therefore the tradeoff bias/variance.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 1</b><br>\n",
    "      Complete the following code to compare the influence of the number of tree on both the bias and the variance. Complete the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m estim_list.append(n_estimators)\n\u001b[32m     35\u001b[39m model = XGBRegressor(n_estimators=n_estimators, max_depth=\u001b[32m3\u001b[39m, learning_rate=\u001b[32m0.1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m y_pred = model.predict(X_test)\n\u001b[32m     38\u001b[39m y_pred_train = model.predict(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/xgboost/sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/xgboost/core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def ground_truth(x):\n",
    "    \"\"\"Ground truth -- function to approximate\"\"\"\n",
    "    return x * np.sin(x) + np.sin(2 * x)\n",
    "\n",
    "def gen_data(n_samples=200):\n",
    "    \"\"\"generate training and testing data\"\"\"\n",
    "    np.random.seed(13)\n",
    "    x = np.random.uniform(0, 10, size=n_samples)\n",
    "    x.sort()\n",
    "    y = ground_truth(x) + 0.75 * np.random.normal(size=n_samples)\n",
    "    train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
    "    x_train, y_train = x[train_mask, np.newaxis], y[train_mask]\n",
    "    x_test, y_test = x[~train_mask, np.newaxis], y[~train_mask]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_data(200)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    model = XGBRegressor(n_estimators=n_estimators, max_depth=3, learning_rate=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    test_list.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list, label='Test',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "ax.annotate('... bias', xy=(900, train_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.3), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... variance', xy=(900, test_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.4), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "One of the most usefull technic for regularization in Gradient Boosted Trees is the Shrinkage : the idea is to slow down the training by reducing the prediction of each tree by a scalar - the learning rate. In this way, the model must produce stronger concepts. \n",
    "\n",
    "A low learning rate impose a greater number of trees (n_estimators) to have a similar error during training - we are trading training time for some more precision.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 2</b><br>\n",
    "      Reuse and modify the previous code to compare two differente learning rates : 1.0 and 0.1. Conclude by completing the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "test_list_2 = []\n",
    "train_list_2 = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    est = ...\n",
    "    \n",
    "    est2 = ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list_2, label='Test 0.1',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list_2, label='Train 0.1', linewidth=2)\n",
    "\n",
    "ax.plot(estim_list, test_list, label='Test 1.0',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train 1.0', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Requires ... trees', xy=(200, train_list_2[199]), xycoords='data',\n",
    "            xytext=(300, 1.0), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... test error', xy=(900, test_list_2[899]), xycoords='data',\n",
    "            xytext=(600, 0.5), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "\n",
    "As for many other algorithms, introducing randomness during training can lead to a greater precision at the end. There are two main parameters that introduce randomness :\n",
    "* We can subsample the training data before growing each tree\n",
    "* We can subsample the features before choosing the best split node\n",
    "\n",
    "[XGBoost list of parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 3</b><br>\n",
    "      In XGBoost list of parameters, find the one that control randomness during training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sources :\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
